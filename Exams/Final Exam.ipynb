{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "main.header"
    ]
   },
   "source": [
    "# `Final Exam, Fall 2024`: `Predicting Student College Enrollment`\n",
    "_Version 1.0.0_\n",
    "\n",
    "*All of the header information is important. Please read it..*\n",
    "\n",
    "**Topics number of exercises:** This problem builds on your knowledge of `['Basic Python', 'Pandas', 'Numpy', 'math as code', 'data cleaning', 'feature engineering', 'logistic regression', 'model evaluation']`. It has **9** exercises numbered 0 to **8**. There are **17** available points. However to earn 100% the threshold is **13** points. (Therefore once you hit **13** points you can stop. There is no extra credit for exceeding this threshold.)\n",
    "\n",
    "**Exercise ordering:** Each exercise builds logically on previous exercises but you may solve them in any order. That is if you can't solve an exercise you can still move on and try the next one. Use this to your advantage as the exercises are **not** necessarily ordered in terms of difficulty. Higher point values generally indicate more difficult exercises. \n",
    "\n",
    "**Demo cells:** Code cells starting with the comment `### Run Me!!!` load results from prior exercises applied to the entire data set and use those to build demo inputs. These must be run for subsequent demos to work properly but they do not affect the test cells. The data loaded in these cells may be rather large (at least in terms of human readability). You are free to print or otherwise use Python to explore them but we may not print them in the starter code.\n",
    "\n",
    "**Debugging your code:** Right before each exercise test cell there is a block of text explaining the variables available to you for debugging. You may use these to test your code and can print/display them as needed (careful when printing large objects you may want to print the head or chunks of rows at a time).\n",
    "\n",
    "**Exercise point breakdown:**\n",
    "\n",
    "\n",
    "- Exercise 0 - : **1** point(s)\n",
    "\n",
    "- Exercise 1 - : **2** point(s)\n",
    "\n",
    "- Exercise 2 - : **3** point(s)\n",
    "\n",
    "- Exercise 3 - : **1** point(s)\n",
    "\n",
    "- Exercise 4 - : **2** point(s)\n",
    "\n",
    "- Exercise 5 - : **1** point(s)\n",
    "\n",
    "- Exercise 6 - : **3** point(s)\n",
    "\n",
    "- Exercise 7 - : **2** point(s)\n",
    "\n",
    "- Exercise 8 - : **2** point(s)\n",
    "\n",
    "\n",
    "**Final reminders:** \n",
    "\n",
    "- Submit after **every exercise**\n",
    "- Review the generated grade report after you submit to see what errors were returned\n",
    "- Stay calm, skip problems as needed and take short breaks at your leisure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam Introduction\n",
    "\n",
    "**Your overall task.** In this exam, you will work with a dataset containing metadata for students admitted to a small liberal arts college. The college seeks to predict which students are likely to commit to enrolling. This is critical for meeting enrollment targets and allocating resources effectively. The target variable in this dataset is the `Gross Commit Indicator`, which has two possible values: 1 if the student commits and 0 if the student does not commit. Your goal is to develop a logistic regression model to predict these outcomes.\n",
    "\n",
    "**Overview:** You will follow a structured workflow to process the data, engineer features, and build a logistic regression model. The notebook is organized into three main sections:\n",
    "\n",
    "1. **Data Exploration and Cleaning:**\n",
    "   - Explore the dataset to understand its structure, key features, and any potential issues such as missing or inconsistent data.\n",
    "   - Clean the dataset by filling in missing values and standardizing feature formats to prepare it for analysis.\n",
    "\n",
    "2. **Feature Engineering:**\n",
    "   - Add meaningful derived features, such as geographic distances, that can improve the predictive power of the model.\n",
    "   - Transform categorical variables and other features into formats suitable for model building.\n",
    "\n",
    "3. **Model Building and Evaluation:**\n",
    "   - Implement key functions for building a logistic regression model. \n",
    "   - These functions will be integrated into the broader logistic regression pipeline to demonstrate the structure of a complete logistic regression model. \n",
    "   \n",
    "By the end of this exercise, you will understand how to calculate and apply key components of logistic regression, and see the implementation of the training loop provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "main.global_imports"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cse6040_devkit.plugins\n"
     ]
    }
   ],
   "source": [
    "### Global imports\n",
    "import dill\n",
    "from cse6040_devkit import plugins, utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "utils.add_from_file('series_handler', plugins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded admission_df.dill.\n"
     ]
    }
   ],
   "source": [
    "admission_df = utils.load_object_from_publicdata('admission_df.dill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "get_dataframe_FREE.prompt"
    ]
   },
   "source": [
    "### Exercise 0: (1 points)\n",
    "**get_dataframe_FREE**  \n",
    "\n",
    "**Example:** we have defined `get_dataframe_FREE` as follows:\n",
    "\n",
    "The exercise is meant to introduce you to the data used throughout the exam. You will receive a free point for completing this task. Run the provided code cell to explore the dataset and familiarize yourself with its structure. \n",
    "\n",
    "**Input:**\n",
    "- `df`: A Pandas DataFrame containing student admission data. This dataset includes features such as GPA, financial aid information, and extracurricular interests. (For this exercise, the input `df` is the `admission_df` provided by the exam.)\n",
    "\n",
    "**Output:**\n",
    "- Displays the first few rows of the DataFrame and the data types for each column.\n",
    "\n",
    "**The dataset:** The dataset comes from the admissions office and includes several features related to students. The goal is to predict the `Gross Commit Indicator`, which indicates whether a student accepted the admission offer.\n",
    "\n",
    "**Dataset Features:** Below are the key features included in the dataset:\n",
    "\n",
    "| Column Name               | Description                              |\n",
    "|---------------------------|------------------------------------------|\n",
    "| Gross Commit Indicator  | Indicates if the student accepted the offer (1) or not (0) |\n",
    "| Financial Aid Intent    | Indicates if the student applied for financial aid |\n",
    "| Scholarship             | Type of scholarship received, if any    |\n",
    "| Direct Legacy? (parent) | Indicates if the student is a direct legacy |\n",
    "| Ethnic Background       | Student's ethnic background              |\n",
    "| First Gen to College    | Indicates if the student is the first generation to attend college |\n",
    "| Permanent Region        | Student's permanent region               |\n",
    "| GPA                     | Student's GPA                             |\n",
    "| HS Class Size           | Size of the student's high school class   |\n",
    "| Campus Visit Indicator? | Indicates if the student visited the campus |\n",
    "| Interview?              | Indicates if the student had an interview |\n",
    "| Sex                     | Student's sex                             |\n",
    "| Level of Financial Need | Indicates the student's level of financial need |\n",
    "| Reader Academic Rating  | Rating given by the admissions reader     |\n",
    "\n",
    "**Instructions for this exercise:**\n",
    "1. Run the provided test cell, which will execute the `get_dataframe_FREE` function using the predefined `admission_df`.\n",
    "2. Review the dataset by examining the printed output. Pay close attention to:\n",
    "   - The features (columns) available in the dataset.\n",
    "   - The presence of missing values or unexpected data types.\n",
    "3. Use this information to guide your understanding of subsequent exercises, which involve cleaning, transforming, and analyzing this dataset.\n",
    "4. No further action is required; completing this step awards you a free point!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "get_dataframe_FREE.solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gross Commit Indicator</th>\n",
       "      <th>Financial Aid Intent</th>\n",
       "      <th>Scholarship</th>\n",
       "      <th>Direct Legacy? (parent)</th>\n",
       "      <th>Ethnic Background</th>\n",
       "      <th>First Gen to College</th>\n",
       "      <th>Permanent Region</th>\n",
       "      <th>GPA</th>\n",
       "      <th>HS Class Size</th>\n",
       "      <th>Campus Visit Indicator?</th>\n",
       "      <th>Interview?</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Level of Financial Need</th>\n",
       "      <th>Reader Academic Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAY</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>3.67</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>Low OxyS - $1 to $19,999</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>FAY</td>\n",
       "      <td>LDRS</td>\n",
       "      <td>0</td>\n",
       "      <td>Black</td>\n",
       "      <td>0</td>\n",
       "      <td>NY</td>\n",
       "      <td>3.76</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>Very High OxyS - $46,000 +</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>FAY</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>Asian</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>3.58</td>\n",
       "      <td>642.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Medium OxyS - $20,000 - $29,999</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>FAY</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>4.00</td>\n",
       "      <td>303.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Low OxyS - $1 to $19,999</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>FAY</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "      <td>0</td>\n",
       "      <td>CA</td>\n",
       "      <td>3.57</td>\n",
       "      <td>386.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>Low OxyS - $1 to $19,999</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gross Commit Indicator Financial Aid Intent Scholarship  \\\n",
       "0                       0                  FAY          NO   \n",
       "1                       0                  FAY        LDRS   \n",
       "2                       0                  FAY          NO   \n",
       "3                       0                  FAY          NO   \n",
       "4                       0                  FAY          NO   \n",
       "\n",
       "   Direct Legacy? (parent) Ethnic Background  First Gen to College  \\\n",
       "0                        0             White                     0   \n",
       "1                        0             Black                     0   \n",
       "2                        0             Asian                     0   \n",
       "3                        0             White                     0   \n",
       "4                        0             White                     0   \n",
       "\n",
       "  Permanent Region   GPA  HS Class Size  Campus Visit Indicator?  Interview?  \\\n",
       "0               CA  3.67           80.0                        1           1   \n",
       "1               NY  3.76           26.0                        1           0   \n",
       "2               CA  3.58          642.0                        1           0   \n",
       "3               CA  4.00          303.0                        1           0   \n",
       "4               CA  3.57          386.0                        1           0   \n",
       "\n",
       "  Sex          Level of Financial Need  Reader Academic Rating  \n",
       "0   M         Low OxyS - $1 to $19,999                       3  \n",
       "1   M       Very High OxyS - $46,000 +                       2  \n",
       "2   F  Medium OxyS - $20,000 - $29,999                       3  \n",
       "3   F         Low OxyS - $1 to $19,999                       2  \n",
       "4   M         Low OxyS - $1 to $19,999                       4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gross Commit Indicator       int64\n",
       "Financial Aid Intent        string\n",
       "Scholarship                 string\n",
       "Direct Legacy? (parent)      int64\n",
       "Ethnic Background           string\n",
       "First Gen to College         int64\n",
       "Permanent Region            string\n",
       "GPA                        float64\n",
       "HS Class Size              float64\n",
       "Campus Visit Indicator?      int64\n",
       "Interview?                   int64\n",
       "Sex                         string\n",
       "Level of Financial Need     string\n",
       "Reader Academic Rating       int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Solution - Exercise 0  \n",
    "def get_dataframe_FREE(df):\n",
    "    return df.head()\n",
    "\n",
    "### Demo function call\n",
    "df_head = get_dataframe_FREE(admission_df)\n",
    "display(df_head)\n",
    "display(df_head.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "get_dataframe_FREE.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    " <!-- Test Cell Boilerplate -->  \n",
    " The test cell below will always pass. Please submit to collect your free points for get_dataframe_FREE (exercise 0).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_0",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "get_dataframe_FREE.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 0  \n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "data_cleaning_and_standardization.prompt"
    ]
   },
   "source": [
    "### Exercise 1: (2 points)\n",
    "**data_cleaning_and_standardization**  \n",
    "\n",
    "**Your task:** define `data_cleaning_and_standardization` as follows:\n",
    "\n",
    "Process the input DataFrame by:\n",
    "  - Filling missing values in specific columns based on predefined strategies.\n",
    "  - Standardizing the categories in the `Level of Financial Need` column for consistency.\n",
    "\n",
    "**Input:**\n",
    "  - `df`: A Pandas DataFrame containing student admission data. (For this exercise, the input `df` is the provided `admission_df` dataframe.)\n",
    "\n",
    "**Output:**\n",
    "  - A modified Pandas DataFrame where missing values are filled and the `Level of Financial Need` column is standardized.\n",
    "\n",
    "**Requirements/steps:**\n",
    "  1. **Fill Missing Values:**\n",
    "    - Replace missing values in `Scholarship` with the string `\"No Scholarship\"`.\n",
    "    - Replace missing values in `Permanent Region` with the string `\"Unknown\"`.\n",
    "    - Replace missing values in `GPA` with the median of non-missing GPA values.\n",
    "    - Replace missing values in `HS Class Size` with the median of non-missing class sizes.\n",
    "    - Replace missing values in `Level of Financial Need` with the string `\"Unknown\"`.\n",
    "\n",
    "  2. **Standardize `Level of Financial Need`:**\n",
    "    - Replace the existing labels in `Level of Financial Need` with the following simplified categories:\n",
    "\n",
    "| Existing Label                     | Simplified Label  |\n",
    "|------------------------------------|-------------------|\n",
    "| `No OxyS - $0`                     | No                |\n",
    "| `Low OxyS - $1 to $19,999`         | Low               |\n",
    "| `Medium OxyS - $20,000 - $29,999`  | Medium            |\n",
    "| `High OxyS - $30,000 to $45,999`   | High              |\n",
    "| `Very High OxyS - $46,000 +`       | Very High         |\n",
    "| `Unknown - In Progress`            | Unknown           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "data_cleaning_and_standardization.solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scholarship</th>\n",
       "      <th>Permanent Region</th>\n",
       "      <th>GPA</th>\n",
       "      <th>HS Class Size</th>\n",
       "      <th>Level of Financial Need</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Scholarship</td>\n",
       "      <td>CA</td>\n",
       "      <td>3.8</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UPBW</td>\n",
       "      <td>NY</td>\n",
       "      <td>3.6</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Scholarship</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>3.9</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Very High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DUN</td>\n",
       "      <td>CA</td>\n",
       "      <td>3.7</td>\n",
       "      <td>250.0</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Scholarship</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>3.5</td>\n",
       "      <td>300.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scholarship Permanent Region  GPA  HS Class Size Level of Financial Need\n",
       "0  No Scholarship               CA  3.8          250.0                     Low\n",
       "1            UPBW               NY  3.6          250.0                  Medium\n",
       "2  No Scholarship          Unknown  3.9          150.0               Very High\n",
       "3             DUN               CA  3.7          250.0                 Unknown\n",
       "4  No Scholarship          Unknown  3.5          300.0                      No"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Solution - Exercise 1  \n",
    "def data_cleaning_and_standardization(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # INPUT: dataframe\n",
    "    # remember to make a copy\n",
    "    \n",
    "    # GOAL: cleaned dataframe with missing values filled and column standardized\n",
    "    \n",
    "    # STRATEGY:\n",
    "    # 1. df.fillna()\n",
    "        # - in [Scholarship]: use string  \"No Scholarship\"\n",
    "        # - in [Permanent Region]: use string \"Unknown\"\n",
    "        # - in [GPA]: use median (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.median.html) df.median(skipna=True)\n",
    "        # - in [HS Class Size]: use median (see above)\n",
    "        # - in [Level of Financial]: use string \"Unknown\"\n",
    "    # 2. df.replace()\n",
    "        # - to_replacestr, regex, list, dict, Series, int, float, or None (use dict)\n",
    "\n",
    "    ###\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    cleaned_df[\"Scholarship\"] = cleaned_df[\"Scholarship\"].fillna(\"No Scholarship\")\n",
    "    cleaned_df[\"Permanent Region\"] = cleaned_df[\"Permanent Region\"].fillna(\"Unknown\")\n",
    "    cleaned_df[\"GPA\"] = cleaned_df[\"GPA\"].fillna(cleaned_df[\"GPA\"].median(skipna=True))\n",
    "    cleaned_df[\"HS Class Size\"] = cleaned_df[\"HS Class Size\"].fillna(cleaned_df[\"HS Class Size\"].median(skipna=True))\n",
    "    cleaned_df[\"Level of Financial Need\"] = cleaned_df[\"Level of Financial Need\"].fillna(\"Unknown\")\n",
    "    \n",
    "    stan_dict = {\n",
    "        \"No OxyS - $0\" : \"No\",\n",
    "        \"Low OxyS - $1 to $19,999\" : \"Low\",\n",
    "        \"Medium OxyS - $20,000 - $29,999\" : \"Medium\",\n",
    "        \"High OxyS - $30,000 to $45,999\" : \"High\",\n",
    "        \"Very High OxyS - $46,000 +\" : \"Very High\",\n",
    "        \"Unknown - In Progress\" : \"Unknown\"}\n",
    "    \n",
    "    cleaned_df[\"Level of Financial Need\"] = cleaned_df[\"Level of Financial Need\"].replace(to_replace=stan_dict)\n",
    "    \n",
    "    return cleaned_df\n",
    "    ###\n",
    "\n",
    "### Demo function call\n",
    "demo_df_data_cleaning_and_standardization = pd.DataFrame({\n",
    "    'Scholarship': [np.nan, 'UPBW', np.nan, 'DUN', np.nan],\n",
    "    'Permanent Region': ['CA', 'NY', np.nan, 'CA', np.nan],\n",
    "    'GPA': [3.8, 3.6, 3.9, np.nan, 3.5],\n",
    "    'HS Class Size': [np.nan, 250, 150, np.nan, 300],\n",
    "    'Level of Financial Need': ['Low OxyS - $1 to $19,999', 'Medium OxyS - $20,000 - $29,999', 'Very High OxyS - $46,000 +', 'Unknown - In Progress', 'No OxyS - $0']\n",
    "})\n",
    "\n",
    "demo_output_data_cleaning_and_standardization = data_cleaning_and_standardization(demo_df_data_cleaning_and_standardization)\n",
    "display(demo_output_data_cleaning_and_standardization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "data_cleaning_and_standardization.test_boilerplate"
    ]
   },
   "source": [
    " **Example:** A correct implementation should produce the following output for the provided demo DataFrame:\n",
    "\n",
    "|     | Scholarship    | Permanent Region |  GPA | HS Class Size | Level of Financial Need |\n",
    "|:---:|:--------------:|:----------------:|:----:|:-------------:|:-----------------------:|\n",
    "| 0   | No Scholarship |               CA |  3.8 |         250.0 |                    Low  |\n",
    "| 1   |           UPBW |               NY |  3.6 |         250.0 |                 Medium  |\n",
    "| 2   | No Scholarship |          Unknown |  3.9 |         150.0 |              Very High  |\n",
    "| 3   |            DUN |               CA |  3.7 |         250.0 |                Unknown  |\n",
    "| 4   | No Scholarship |          Unknown |  3.5 |         300.0 |                     No  |\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for data_cleaning_and_standardization (exercise 1). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_1",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "data_cleaning_and_standardization.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_cleaning_and_standardization test ran 50 iterations in 8.37 seconds\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 1  \n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    execute_tests = dill.load(f)\n",
    "\n",
    "# Execute test\n",
    "passed, test_case_vars = execute_tests(func=data_cleaning_and_standardization,\n",
    "              ex_name='data_cleaning_and_standardization',\n",
    "              key=b'apvdqSXE1hpoezgeyhgb6Y557k-QtNd5WaF1QCOuIQE=', \n",
    "              n_iter=50)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "\n",
    "assert passed, 'The solution to data_cleaning_and_standardization did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "state_coordinates.prompt"
    ]
   },
   "source": [
    "### Exercise 2: (3 points)\n",
    "**state_coordinates**  \n",
    "\n",
    "**Your task:** define `state_coordinates` as follows:\n",
    "\n",
    "We need to tie together state data that exists in separate dictionaries. \n",
    "    `abbr_dict` holds each state's abbreviation and state name. \n",
    "    `coor_dict` hold each state name and the coordinates. This data needs to be joined so we have a single data structure that\n",
    "    holds each state name, the coordinates, and the state abbreviation.\n",
    "\n",
    "**Input:**\n",
    "  - `abbr_dict`: A dictionary with each key the state abbreviation and each value the respective state name, each in lowercase. \n",
    "  - `coor_dict`: A nested dictionary with the outer keys of `latitude` and `longitude`. Each inner dictionary holds the full state name (for example 'California') as the key and the respective coordinate float as the value.\n",
    "\n",
    "**Output:**\n",
    "  - `state_data`: A new list of dictionaries where each dictionary holds the data for 1 state, **sorted by the state full name**. Within each dictionary will be the following key-value pairs:\n",
    "    - `state`: The value of this key is the full name of the respective state (for example, 'California') as a string\n",
    "    - `latitude`: The value of this key is the latitude as a float\n",
    "    - `longtitude`: The value of this key is the longtitude as a float\n",
    "    - `abbr`: The value of this key is the upper-case abbreviation of the respective state (for example 'CA') as a string\n",
    "\n",
    "**Additional Notes:**\n",
    "  - `coor_dict` is guaranteed to have the keys `latitude` and `longitude`.\n",
    "  - The `latitude` and `longitude` values in `coor_dict` are expected to be floats. If there are data type inconsistencies, they should be handled appropriately to maintain the expected format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "state_coordinates.solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'state': 'California',\n",
       "  'latitude': 36.17,\n",
       "  'longitude': -119.7462,\n",
       "  'abbr': 'CA'},\n",
       " {'state': 'North Carolina',\n",
       "  'latitude': 35.6411,\n",
       "  'longitude': -79.8431,\n",
       "  'abbr': 'NC'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Solution - Exercise 2  \n",
    "def state_coordinates(abbr_dict: dict, coor_dict: dict) -> dict:\n",
    "    \n",
    "    # INPUT: two dictionaries\n",
    "    # abbr_dict: maps abbrieviation to name eg. {abbreviation: name, abbreviation2: name2, etc...}\n",
    "    # coor_dict: maps coordinates to name eg.{latitude: {}, longtitude: {}}\n",
    "    \n",
    "    # GOAL: list of dictionaries, one dictionary per state, sorted by state full name (sorted by first value), coords should be floats\n",
    "    # eg. {state: value1, latitude: value2, longtitude: value3, abbr: value4}\n",
    "    \n",
    "    # STRATEGY:\n",
    "    # 1. Loop through abbr_dict to get the state\n",
    "    # 2. Once we have the state we can look for it in coor_dict\n",
    "        # 2a. look in latitude[state name]\n",
    "        # 2b. look in longtitude[state name]\n",
    "    # 3. append to output list\n",
    "    # 4. sort output list newlist = sorted(list_to_be_sorted, key=lambda d: d['name'])\n",
    "    # https://stackoverflow.com/questions/72899/how-to-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary-in-python\n",
    "\n",
    "    ###\n",
    "    import string\n",
    "\n",
    "    \n",
    "    state_list = []\n",
    "    \n",
    "    for key,val in abbr_dict.items():\n",
    "        state = string.capwords(val) # we have to capitalize every word in this\n",
    "        abbr = key.upper()\n",
    "        latitude = float(coor_dict[\"latitude\"][state])\n",
    "        longitude = float(coor_dict[\"longitude\"][state])\n",
    "    \n",
    "        state_dict = {\"state\" : state, \"latitude\" : latitude, \"longitude\" : longitude, \"abbr\" : abbr}\n",
    "    \n",
    "        state_list.append(state_dict)\n",
    "    \n",
    "    state_list = sorted(state_list, key=lambda d: d[\"state\"])\n",
    "    \n",
    "    return state_list\n",
    "    ###\n",
    "\n",
    "### Demo function call\n",
    "demo_abbr_dict_state_coordinates = {'nc': 'north carolina', 'ca': 'california'}\n",
    "demo_door_dict_state_coordinates = {'latitude': {'California': 36.17, 'North Carolina': 35.6411}, \n",
    "                                    'longitude': {'California': -119.7462, 'North Carolina': -79.8431}}\n",
    "\n",
    "demo_output_state_coordinates = state_coordinates(demo_abbr_dict_state_coordinates, demo_door_dict_state_coordinates)\n",
    "display(demo_output_state_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "state_coordinates.test_boilerplate"
    ]
   },
   "source": [
    " **Example:** A correct implementation should produce the following output for the provided demo DataFrame:\n",
    "\n",
    "```python\n",
    "[{'state': 'California',\n",
    "  'latitude': 36.17,\n",
    "  'longitude': -119.7462,\n",
    "  'abbr': 'CA'},\n",
    "  {'state': 'North Carolina',\n",
    "  'latitude': 35.6411,\n",
    "  'longitude': -79.8431,\n",
    "  'abbr': 'NC'}]\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for state_coordinates (exercise 2). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_2",
     "locked": true,
     "points": 3,
     "solution": false
    },
    "tags": [
     "state_coordinates.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_coordinates test ran 100 iterations in 0.21 seconds\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 2  \n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    execute_tests = dill.load(f)\n",
    "\n",
    "# Execute test\n",
    "passed, test_case_vars = execute_tests(func=state_coordinates,\n",
    "              ex_name='state_coordinates',\n",
    "              key=b'apvdqSXE1hpoezgeyhgb6Y557k-QtNd5WaF1QCOuIQE=', \n",
    "              n_iter=100)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "\n",
    "assert passed, 'The solution to state_coordinates did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "add_lat_long.prompt"
    ]
   },
   "source": [
    "### Exercise 3: (1 points)\n",
    "**add_lat_long**  \n",
    "\n",
    "**Your task:** define `add_lat_long` as follows:\n",
    "\n",
    "Add two new columns to the input DataFrame (`latitude` and `longitude`), based on the `Permanent Region` column. Use the provided `state_data` dictionary to map state abbreviations to their corresponding geographic coordinates.\n",
    "\n",
    "**Input:**\n",
    "  - `df`: A Pandas DataFrame. It must include the column `Permanent Region`, which contains state abbreviations (e.g., `\"CA\"`, `\"NY\"`).\n",
    "  (For this exercise, the input `df` is the `admission_df` modified through prior steps.)\n",
    "  - `state_data`: A list of dictionaries, where each dictionary represents a state and includes the following fields:\n",
    "    - `\"abbr\"`: State abbreviation (e.g., `\"CA\"`, `\"NY\"`)\n",
    "    - `\"latitude\"`: Latitude of the state (float)\n",
    "    - `\"longitude\"`: Longitude of the state (float)\n",
    "\n",
    "**Output:**\n",
    "  - A new Pandas DataFrame with two new columns added.:\n",
    "    - `latitude`: Latitude of the state based on `Permanent Region`\n",
    "    - `longitude`: Longitude of the state based on `Permanent Region`\n",
    "\n",
    "**Requirements/steps:**\n",
    "  1. Create a mapping from state abbreviations to their corresponding latitude and longitude using `state_data`.\n",
    "   For example, the dictionary `{ \"CA\": {\"latitude\": 36.7783, \"longitude\": -119.4179} }` maps `\"CA\"` to its coordinates.\n",
    "  2. Use this mapping to populate the `latitude` and `longitude` columns based on the `Permanent Region` column in `df`.\n",
    "  3. If a value in `Permanent Region` doesn't match any key in the mapping, set `latitude` and `longitude` to `pd.NA`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "add_lat_long.solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Permanent Region</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FL</td>\n",
       "      <td>27.8333</td>\n",
       "      <td>-81.717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NY</td>\n",
       "      <td>42.1497</td>\n",
       "      <td>-74.9384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA</td>\n",
       "      <td>36.17</td>\n",
       "      <td>-119.7462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GA</td>\n",
       "      <td>32.9866</td>\n",
       "      <td>-83.6487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FR</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Permanent Region latitude longitude\n",
       "0               FL  27.8333   -81.717\n",
       "1               NY  42.1497  -74.9384\n",
       "2               CA    36.17 -119.7462\n",
       "3               GA  32.9866  -83.6487\n",
       "4               FR     <NA>      <NA>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Solution - Exercise 3  \n",
    "def add_lat_long(df: pd.DataFrame, state_data) -> pd.DataFrame:\n",
    "    \n",
    "    # INPUT: dataframe + state_data (list of dictionaries)\n",
    "    # GOAL: dataframe with two columns for longitude and latitude based on [Permanent Region] state (displayed as an abbr)\n",
    "    # STRATEGY: \n",
    "    # store as list > convert to dataframe > append to main dataframe\n",
    "    # fillna(pd.NA)\n",
    "    \n",
    "    ###\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    PR_list = output_df[\"Permanent Region\"].tolist()\n",
    "    latitude_list = []\n",
    "    longitude_list = []\n",
    "    \n",
    "    for state in PR_list:\n",
    "        pinged_dict = next((item for item in state_data if item[\"abbr\"] == state), None)\n",
    "        \n",
    "        if pinged_dict:\n",
    "            latitude_list.append(pinged_dict[\"latitude\"])\n",
    "            longitude_list.append(pinged_dict[\"longitude\"])\n",
    "        else:\n",
    "            latitude_list.append(pd.NA)\n",
    "            longitude_list.append(pd.NA)\n",
    "    \n",
    "    coordinates_dict = {\"latitude\" : latitude_list, \"longitude\" : longitude_list}\n",
    "    coordinates_df = pd.DataFrame(coordinates_dict)\n",
    "    \n",
    "    output_df = pd.concat([output_df, coordinates_df], axis=1)\n",
    "    \n",
    "    return output_df\n",
    "    ###\n",
    "\n",
    "### Demo function call\n",
    "demo_df_add_lat_long = pd.DataFrame({\"Permanent Region\": [\"FL\", \"NY\", \"CA\", \"GA\", \"FR\"]})\n",
    "demo_state_data = [\n",
    "    {\"state\": \"Florida\", \"latitude\": 27.8333, \"longitude\": -81.717, \"abbr\": \"FL\"},\n",
    "    {\"state\": \"New York\", \"latitude\": 42.1497, \"longitude\": -74.9384, \"abbr\": \"NY\"},\n",
    "    {\"state\": \"California\", \"latitude\": 36.17, \"longitude\": -119.7462, \"abbr\": \"CA\"},\n",
    "    {\"state\": \"Georgia\", \"latitude\": 32.9866, \"longitude\": -83.6487, \"abbr\": \"GA\"}\n",
    "]\n",
    "demo_output_add_lat_long = add_lat_long(demo_df_add_lat_long, demo_state_data)\n",
    "display(demo_output_add_lat_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "add_lat_long.test_boilerplate"
    ]
   },
   "source": [
    " **Example:** A correct implementation should produce the following output:\n",
    "\n",
    "| Permanent Region |  latitude |   longitude |\n",
    "|:----------------:|:---------:|:-----------:|\n",
    "| FL               |   27.8333 |     -81.717 |\n",
    "| NY               |   42.1497 |    -74.9384 |\n",
    "| CA               |     36.17 |   -119.7462 |\n",
    "| GA               |   32.9866 |    -83.6487 |\n",
    "| FR               |    `<NA>` |      `<NA>` |\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for add_lat_long (exercise 3). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_3",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "add_lat_long.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_lat_long test ran 100 iterations in 0.64 seconds\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 3  \n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    execute_tests = dill.load(f)\n",
    "\n",
    "# Execute test\n",
    "passed, test_case_vars = execute_tests(func=add_lat_long,\n",
    "              ex_name='add_lat_long',\n",
    "              key=b'apvdqSXE1hpoezgeyhgb6Y557k-QtNd5WaF1QCOuIQE=', \n",
    "              n_iter=100)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "\n",
    "assert passed, 'The solution to add_lat_long did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "calculate_distance.prompt"
    ]
   },
   "source": [
    "### Exercise 4: (2 points)\n",
    "**calculate_distance**  \n",
    "\n",
    "**Your task:** define `calculate_distance` as follows:\n",
    "\n",
    "Calculate the distance in miles from each student's location \n",
    "to the school. The location of each student is determined by the `latitude` and `longitude` columns in the input DataFrame, \n",
    "while the school's location is specified by the coordinates latitude `34.1271` and longitude `-118.2109`.\n",
    "\n",
    "**Input:**\n",
    "  - `df`: A Pandas DataFrame containing two columns:\n",
    "    - `latitude`: Latitude of the student's location in decimal degrees.\n",
    "    - `longitude`: Longitude of the student's location in decimal degrees.\n",
    "\n",
    "**Output:**\n",
    "  - A new Pandas DataFrame that includes all columns from the input `df`, with an additional column:\n",
    "    - `distance_from_school`: The distance (in miles) from each student's location to the school.\n",
    "\n",
    "**Requirements/steps:**\n",
    "  1. **Convert Columns to Numeric:**\n",
    "     - Use the `pd.to_numeric` function to convert the `latitude` and `longitude` columns to numeric values and coerce any errors. This step ensures that non-numeric or missing values are converted to `NaN`.\n",
    "  2. **Convert Coordinates to Radians:**\n",
    "     - Use the `np.radians` function to convert all latitude and longitude values (both student and school) from degrees to radians.\n",
    "  3. **Apply the Haversine Formula:**\n",
    "     - Compute the distance between each student's location and the school's location using the following formula:\n",
    "\n",
    "       $$d = 2r \\cdot \\arcsin \\left( \\sqrt{\\sin^2 \\left( \\frac{\\Delta \\phi}{2} \\right) + \\cos(\\phi_1) \\cdot \\cos(\\phi_2) \\cdot \\sin^2 \\left( \\frac{\\Delta \\lambda}{2} \\right)} \\right)$$\n",
    "\n",
    "     - Definitions:\n",
    "       - $\\phi_1$ and $\\phi_2$ are the latitudes (in radians) of the school and the student, respectively.\n",
    "       - $\\lambda_1$ and $\\lambda_2$ are the longitudes (in radians) of the school and the student, respectively.\n",
    "       - $\\Delta \\phi = \\phi_2 - \\phi_1$ (difference in latitudes).\n",
    "       - $\\Delta \\lambda = \\lambda_2 - \\lambda_1$ (difference in longitudes).\n",
    "       - $r$ is the Earth's radius (mean radius = 3,959 miles).\n",
    "  4. **Add the New Column:**\n",
    "     - Create a new column, `distance_from_school`, to store the calculated distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "calculate_distance.solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Permanent Region</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>distance_from_school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FL</td>\n",
       "      <td>27.8333</td>\n",
       "      <td>-81.7170</td>\n",
       "      <td>2193.210034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NY</td>\n",
       "      <td>42.1497</td>\n",
       "      <td>-74.9384</td>\n",
       "      <td>2389.334519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA</td>\n",
       "      <td>36.1700</td>\n",
       "      <td>-119.7462</td>\n",
       "      <td>165.674547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GA</td>\n",
       "      <td>32.9866</td>\n",
       "      <td>-83.6487</td>\n",
       "      <td>1982.193883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Permanent Region  latitude  longitude  distance_from_school\n",
       "0               FL   27.8333   -81.7170           2193.210034\n",
       "1               NY   42.1497   -74.9384           2389.334519\n",
       "2               CA   36.1700  -119.7462            165.674547\n",
       "3               GA   32.9866   -83.6487           1982.193883\n",
       "4               TX       NaN        NaN                   NaN"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Solution - Exercise 4  \n",
    "def calculate_distance(df: pd.DataFrame, school_lat: float, school_lon: float) -> pd.DataFrame:\n",
    "    # INPUT: dataframe with state, lat, long\n",
    "    # OUTPUT: dataframe with state, lat, long, distance from school\n",
    "    # STRATEGY: \n",
    "    # 1. convert columns to numeric (pd.to_numeric)\n",
    "    # 2. convert to radians (np.radians)\n",
    "    # 3. newlist = loop through df > apply haversine function\n",
    "    \n",
    "    ###\n",
    "    import math\n",
    "    \n",
    "    output_df = df.copy()\n",
    "    \n",
    "    output_df[\"latitude\"] = pd.to_numeric(output_df[\"latitude\"], errors=\"coerce\")\n",
    "    output_df[\"longitude\"] = pd.to_numeric(output_df[\"longitude\"], errors=\"coerce\")\n",
    "    \n",
    "    sch_lat = 34.1271    # in degrees\n",
    "    sch_lon = -118.2109  # in degrees\n",
    "    \n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        dLat = (lat2 - lat1) * math.pi / 180.0\n",
    "        dLon = (lon2 - lon1) * math.pi / 180.0\n",
    "        lat1 = (lat1) * math.pi / 180.0\n",
    "        lat2 = (lat2) * math.pi / 180.0\n",
    "        a = (pow(math.sin(dLat / 2), 2) +\n",
    "             pow(math.sin(dLon / 2), 2) *\n",
    "                 math.cos(lat1) * math.cos(lat2));\n",
    "        rad = 3959 #6371\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        return rad * c\n",
    "    \n",
    "    dist_list = []\n",
    "    lat_list = output_df[\"latitude\"].tolist()\n",
    "    lon_list = output_df[\"longitude\"].tolist()\n",
    "    \n",
    "    for lat,lon in zip(lat_list, lon_list):\n",
    "        dist = haversine(sch_lat, sch_lon, lat, lon)\n",
    "        dist_list.append(dist)\n",
    "    \n",
    "    output_df[\"distance_from_school\"] = dist_list\n",
    "    \n",
    "    return output_df\n",
    "        \n",
    "    ###\n",
    "\n",
    "### Demo function call\n",
    "demo_df_calculate_distance = pd.DataFrame({\n",
    "    \"Permanent Region\": [\"FL\", \"NY\", \"CA\", \"GA\", \"TX\"],\n",
    "    \"latitude\": [27.8333, 42.1497, 36.17, 32.9866, \"invalid\"],\n",
    "    \"longitude\": [-81.717, -74.9384, -119.7462, -83.6487, \"missing\"]\n",
    "})\n",
    "\n",
    "demo_output_calculate_distance = calculate_distance(\n",
    "    demo_df_calculate_distance, \n",
    "    school_lat=34.1271, \n",
    "    school_lon=-118.2109\n",
    ")\n",
    "demo_output_calculate_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "calculate_distance.test_boilerplate"
    ]
   },
   "source": [
    " **Example:** A correct implementation should produce the following output:\n",
    "\n",
    "|   | Permanent Region |   latitude   |   longitude   |  distance_from_school |\n",
    "|:-:|:----------------:|:------------:|:-------------:|:---------------------:|\n",
    "| 0 |               FL |     27.8333  |     -81.717   |           2193.210034 |\n",
    "| 1 |               NY |     42.1497  |    -74.9384   |           2389.334519 |\n",
    "| 2 |               CA |     36.17    |   -119.7462   |            165.674547 |\n",
    "| 3 |               GA |     32.9866  |    -83.6487   |           1982.193883 |\n",
    "| 4 |               TX |        NaN   |     NaN       |                   NaN |\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for calculate_distance (exercise 4). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_4",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "calculate_distance.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_distance test ran 100 iterations in 0.74 seconds\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 4  \n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    execute_tests = dill.load(f)\n",
    "\n",
    "# Execute test\n",
    "passed, test_case_vars = execute_tests(func=calculate_distance,\n",
    "              ex_name='calculate_distance',\n",
    "              key=b'apvdqSXE1hpoezgeyhgb6Y557k-QtNd5WaF1QCOuIQE=', \n",
    "              n_iter=100)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "\n",
    "assert passed, 'The solution to calculate_distance did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "one_hot_encode.prompt"
    ]
   },
   "source": [
    "### Exercise 5: (1 points)\n",
    "**one_hot_encode**  \n",
    "\n",
    "**Your task:** define `one_hot_encode` as follows:\n",
    "\n",
    "\n",
    "Perform one-hot encoding on a dataset using `pandas.get_dummies`.\n",
    "\n",
    "**Input:**\n",
    "- `df`: A pandas DataFrame containing the columns to be one-hot encoded.\n",
    "\n",
    "**Output:**\n",
    "- `encoded_df`: A pandas DataFrame containing:\n",
    "    - The specified columns one-hot encoded, with new columns for each unique category in the original columns.\n",
    "    - All other columns from the input `df` retained unmodified.\n",
    "\n",
    "**Requirements/steps:**\n",
    "1. One-hot encode the following columns:\n",
    "    - \"Financial Aid Intent\"\n",
    "    - \"Scholarship\"\n",
    "    - \"Ethnic Background\"\n",
    "    - \"Permanent Region\"\n",
    "    - \"Sex\"\n",
    "    - \"Level of Financial Need\"\n",
    "2. Ensure that the resulting one-hot encoded columns are encoded as floats.\n",
    "3. Retain all other columns in the original DataFrame without modification.\n",
    "\n",
    "**Additional Notes:**\n",
    "- Refer to the `pandas.get_dummies` [documentation](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "one_hot_encode.solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Financial Aid Intent_FAN</th>\n",
       "      <th>Financial Aid Intent_FAY</th>\n",
       "      <th>Scholarship_DIR</th>\n",
       "      <th>Scholarship_LDRS</th>\n",
       "      <th>Scholarship_NO</th>\n",
       "      <th>Scholarship_No Scholarship</th>\n",
       "      <th>Ethnic Background_Asian</th>\n",
       "      <th>Ethnic Background_Black</th>\n",
       "      <th>Ethnic Background_Multiracial</th>\n",
       "      <th>Ethnic Background_White</th>\n",
       "      <th>Permanent Region_CA</th>\n",
       "      <th>Permanent Region_NY</th>\n",
       "      <th>Permanent Region_WA</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>Level of Financial Need_High</th>\n",
       "      <th>Level of Financial Need_Low</th>\n",
       "      <th>Level of Financial Need_Medium</th>\n",
       "      <th>Level of Financial Need_Very High</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Financial Aid Intent_FAN  Financial Aid Intent_FAY  Scholarship_DIR  \\\n",
       "0                       0.0                       1.0              0.0   \n",
       "1                       0.0                       1.0              0.0   \n",
       "2                       1.0                       0.0              0.0   \n",
       "3                       0.0                       1.0              0.0   \n",
       "4                       0.0                       1.0              0.0   \n",
       "5                       1.0                       0.0              1.0   \n",
       "6                       1.0                       0.0              1.0   \n",
       "7                       0.0                       1.0              0.0   \n",
       "8                       0.0                       1.0              0.0   \n",
       "9                       1.0                       0.0              1.0   \n",
       "\n",
       "   Scholarship_LDRS  Scholarship_NO  Scholarship_No Scholarship  \\\n",
       "0               0.0             1.0                         0.0   \n",
       "1               1.0             0.0                         0.0   \n",
       "2               0.0             1.0                         0.0   \n",
       "3               0.0             1.0                         0.0   \n",
       "4               0.0             0.0                         1.0   \n",
       "5               0.0             0.0                         0.0   \n",
       "6               0.0             0.0                         0.0   \n",
       "7               0.0             1.0                         0.0   \n",
       "8               0.0             1.0                         0.0   \n",
       "9               0.0             0.0                         0.0   \n",
       "\n",
       "   Ethnic Background_Asian  Ethnic Background_Black  \\\n",
       "0                      0.0                      0.0   \n",
       "1                      0.0                      1.0   \n",
       "2                      1.0                      0.0   \n",
       "3                      0.0                      0.0   \n",
       "4                      0.0                      0.0   \n",
       "5                      0.0                      0.0   \n",
       "6                      0.0                      0.0   \n",
       "7                      0.0                      1.0   \n",
       "8                      1.0                      0.0   \n",
       "9                      0.0                      0.0   \n",
       "\n",
       "   Ethnic Background_Multiracial  Ethnic Background_White  \\\n",
       "0                            0.0                      1.0   \n",
       "1                            0.0                      0.0   \n",
       "2                            0.0                      0.0   \n",
       "3                            0.0                      1.0   \n",
       "4                            1.0                      0.0   \n",
       "5                            0.0                      1.0   \n",
       "6                            0.0                      1.0   \n",
       "7                            0.0                      0.0   \n",
       "8                            0.0                      0.0   \n",
       "9                            0.0                      1.0   \n",
       "\n",
       "   Permanent Region_CA  Permanent Region_NY  Permanent Region_WA  Sex_F  \\\n",
       "0                  1.0                  0.0                  0.0    0.0   \n",
       "1                  0.0                  1.0                  0.0    0.0   \n",
       "2                  1.0                  0.0                  0.0    1.0   \n",
       "3                  1.0                  0.0                  0.0    1.0   \n",
       "4                  1.0                  0.0                  0.0    1.0   \n",
       "5                  0.0                  0.0                  1.0    0.0   \n",
       "6                  0.0                  0.0                  1.0    0.0   \n",
       "7                  0.0                  1.0                  0.0    0.0   \n",
       "8                  1.0                  0.0                  0.0    1.0   \n",
       "9                  0.0                  0.0                  1.0    0.0   \n",
       "\n",
       "   Sex_M  Level of Financial Need_High  Level of Financial Need_Low  \\\n",
       "0    1.0                           0.0                          1.0   \n",
       "1    1.0                           0.0                          0.0   \n",
       "2    0.0                           0.0                          0.0   \n",
       "3    0.0                           0.0                          1.0   \n",
       "4    0.0                           1.0                          0.0   \n",
       "5    1.0                           0.0                          1.0   \n",
       "6    1.0                           0.0                          1.0   \n",
       "7    1.0                           0.0                          0.0   \n",
       "8    0.0                           1.0                          0.0   \n",
       "9    1.0                           0.0                          0.0   \n",
       "\n",
       "   Level of Financial Need_Medium  Level of Financial Need_Very High  \n",
       "0                             0.0                                0.0  \n",
       "1                             0.0                                1.0  \n",
       "2                             1.0                                0.0  \n",
       "3                             0.0                                0.0  \n",
       "4                             0.0                                0.0  \n",
       "5                             0.0                                0.0  \n",
       "6                             0.0                                0.0  \n",
       "7                             1.0                                0.0  \n",
       "8                             0.0                                0.0  \n",
       "9                             0.0                                1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Solution - Exercise 5  \n",
    "def one_hot_encode(df: pd.DataFrame):\n",
    "\n",
    "    ###\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    to_encode = [\"Financial Aid Intent\", \"Scholarship\", \"Ethnic Background\", \"Permanent Region\", \"Sex\", \"Level of Financial Need\"]\n",
    "    \n",
    "    output_df = pd.get_dummies(output_df, columns=to_encode, dtype=\"float\")\n",
    "    \n",
    "    return output_df\n",
    "    \n",
    "    ###\n",
    "\n",
    "### Demo function call\n",
    "demo_df_one_hot_encode = pd.DataFrame({\n",
    "    'Financial Aid Intent': ['FAY', 'FAY', 'FAN', 'FAY', 'FAY', 'FAN', 'FAN', 'FAY', 'FAY', 'FAN'],\n",
    "    'Scholarship': ['NO', 'LDRS', 'NO', 'NO', 'No Scholarship', 'DIR', 'DIR', 'NO', 'NO', 'DIR'],\n",
    "    'Ethnic Background': ['White', 'Black', 'Asian', 'White', 'Multiracial', 'White', 'White', 'Black', 'Asian', 'White'],\n",
    "    'Permanent Region': ['CA', 'NY', 'CA', 'CA', 'CA', 'WA', 'WA', 'NY', 'CA', 'WA'],\n",
    "    'Sex': ['M', 'M', 'F', 'F', 'F', 'M', 'M', 'M', 'F', 'M'],\n",
    "    'Level of Financial Need': ['Low', 'Very High', 'Medium', 'Low', 'High', 'Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "})\n",
    "\n",
    "demo_output_one_hot_encode = one_hot_encode(demo_df_one_hot_encode)\n",
    "display(demo_output_one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "one_hot_encode.test_boilerplate"
    ]
   },
   "source": [
    " **Example:** A correct implementation should produce the following output:\n",
    "\n",
    "| Financial Aid Intent_FAN | Financial Aid Intent_FAY | Scholarship_DIR | Scholarship_LDRS | Scholarship_NO | Scholarship_No Scholarship | Ethnic Background_Asian | Ethnic Background_Black | Ethnic Background_Multiracial | Ethnic Background_White | Permanent Region_CA | Permanent Region_NY | Permanent Region_WA | Sex_F | Sex_M | Level of Financial Need_High | Level of Financial Need_Low | Level of Financial Need_Medium | Level of Financial Need_Very High |\n",
    "|:-------------------------:|:-------------------------:|:---------------:|:----------------:|:--------------:|:--------------------------:|:-----------------------:|:-----------------------:|:-----------------------------:|:-----------------------:|:-------------------:|:-------------------:|:-------------------:|:-----:|:-----:|:----------------------------:|:---------------------------:|:-----------------------------:|:----------------------------------:|\n",
    "|           0.0            |           1.0            |       0.0       |        0.0       |       0.0      |           1.0             |           0.0           |           0.0           |             0.0               |           1.0           |         1.0         |         0.0         |         0.0         |  0.0  |  1.0  |             0.0             |           0.0             |            1.0                |              0.0                |\n",
    "|           0.0            |           1.0            |       0.0       |        1.0       |       0.0      |           0.0             |           0.0           |           1.0           |             0.0               |           0.0           |         0.0         |         1.0         |         0.0         |  0.0  |  1.0  |             0.0             |           0.0             |            0.0                |              1.0                |\n",
    "|           1.0            |           0.0            |       0.0       |        0.0       |       0.0      |           0.0             |           1.0           |           0.0           |             0.0               |           0.0           |         0.0         |         0.0         |         0.0         |  1.0  |  0.0  |             0.0             |           0.0             |            1.0                |              0.0                |\n",
    "|           0.0            |           1.0            |       0.0       |        0.0       |       0.0      |           0.0             |           0.0           |           0.0           |             1.0               |           0.0           |         1.0         |         0.0         |         0.0         |  1.0  |  0.0  |             0.0             |           1.0             |            0.0                |              0.0                |\n",
    "|           0.0            |           1.0            |       0.0       |        0.0       |       0.0      |           0.0             |           1.0           |           0.0           |             0.0               |           0.0           |         0.0         |         0.0         |         1.0         |  1.0  |  0.0  |             1.0             |           0.0             |            0.0                |              0.0                |\n",
    "|           0.0            |           1.0            |       0.0       |        0.0       |       0.0      |           0.0             |           1.0           |           0.0           |             0.0               |           0.0           |         1.0         |         0.0         |         0.0         |  0.0  |  1.0  |             0.0             |           1.0             |            0.0                |              0.0                |\n",
    "|           1.0            |           0.0            |       0.0       |        1.0       |       0.0      |           0.0             |           0.0           |           0.0           |             1.0               |           0.0           |         0.0         |         0.0         |         1.0         |  0.0  |  1.0  |             0.0             |           0.0             |            0.0                |              1.0                |\n",
    "|           1.0            |           0.0            |       1.0       |        0.0       |       0.0      |           0.0             |           0.0           |           0.0           |             0.0               |           1.0           |         1.0         |         0.0         |         0.0         |  0.0  |  1.0  |             1.0             |           0.0             |            0.0                |              0.0                |\n",
    "|           1.0            |           0.0            |       0.0       |        0.0       |       0.0      |           0.0             |           0.0           |           1.0           |             0.0               |           0.0           |         1.0         |         0.0         |         0.0         |  0.0  |  1.0  |             0.0             |           1.0             |            0.0                |              0.0                |\n",
    "|           0.0            |           1.0            |       0.0       |        0.0       |       1.0      |           0.0             |           1.0           |           0.0           |             0.0               |           0.0           |         0.0         |         1.0         |         0.0         |  1.0  |  0.0  |             0.0             |           0.0             |            1.0                |              1.0                |\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for one_hot_encode (exercise 5). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_5",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "one_hot_encode.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_encode test ran 50 iterations in 11.08 seconds\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 5  \n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    execute_tests = dill.load(f)\n",
    "\n",
    "# Execute test\n",
    "passed, test_case_vars = execute_tests(func=one_hot_encode,\n",
    "              ex_name='one_hot_encode',\n",
    "              key=b'apvdqSXE1hpoezgeyhgb6Y557k-QtNd5WaF1QCOuIQE=', \n",
    "              n_iter=50)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "\n",
    "assert passed, 'The solution to one_hot_encode did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "balance_split_data.prompt"
    ]
   },
   "source": [
    "### Exercise 6: (3 points)\n",
    "**balance_split_data**  \n",
    "\n",
    "**Your task:** define `balance_split_data` as follows:\n",
    "\n",
    "Balance the dataset by oversampling the minority class and split the data into training and test datasets. \n",
    "You will use the `train_test_split` and `resample` functions from `sklearn` to complete this task.\n",
    "The functions have been imported for you. Refer to the documentation below for more information.:\n",
    "\n",
    "- [train_test_split documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "- [resample documentation](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html)\n",
    "\n",
    "**Input:** \n",
    "- `df`: A pandas DataFrame containing the student admissions data.\n",
    "- `ind_col`: The indicator column containing the dependent variable encoded as binary: 0 or 1 (`Gross Commit Indicator` in our data).\n",
    "- `test_size`: A float between 0.0 and 1.0 representing the proportion of the dataset to include in the test split.\n",
    "- `random_state`: An integer representing the random state for reproducibility.\n",
    "\n",
    "**Output:** \n",
    "- `X_train`: A pandas DataFrame containing the features for the training dataset.\n",
    "- `X_test`: A pandas DataFrame containing the features for the test dataset.\n",
    "- `y_train`: A pandas Series containing the target variable for the training dataset.\n",
    "- `y_test`: A pandas Series containing the target variable for the test dataset.\n",
    "\n",
    "Each object represents a part of the split balanced dataset, with `X_train` and `X_test` containing the independent variables (features), and `y_train` and `y_test` containing the dependent variable (target). \n",
    "The features in `X_train` and `X_test` retain the same column names as the input DataFrame, excluding the indicator column (`ind_col`) used as the target.\n",
    "\n",
    "**Requirements/steps:**\n",
    "1. **Oversample the Minority Class**:\n",
    "    - Separate the DataFrame into two subsets: one for the minority class (`ind_col` = 1) and one for the majority class (`ind_col` = 0).\n",
    "    - Use `sklearn.utils.resample` to oversample the minority class to match the majority class size.\n",
    "    - Pass the `random_state` parameter to the resampling function for reproducibility.\n",
    "    - Combine the two subsets to create a balanced dataset.\n",
    "2. **Split the Balanced Data**:\n",
    "    - Use `train_test_split` to split the balanced data into training and test sets.\n",
    "    - Pass the `random_state` parameter to the split function for reproducibility.\n",
    "    - Set the `stratify` parameter to maintain the class balance in both splits.\n",
    "3. **Return** the features (`X_train`, `X_test`) and target variables (`y_train`, `y_test`) for both sets.\n",
    "\n",
    "**Notes:**\n",
    "- The same random state value (from the function call) should be used for both the resampling and the split functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": [
     "balance_split_data.solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (8, 21)\n",
      "X_test.shape:  (2, 21)\n",
      "y_train.shape: (8,)\n",
      "y_test.shape:  (2,)\n"
     ]
    }
   ],
   "source": [
    "### Solution - Exercise 6  \n",
    "def balance_split_data(df: pd.DataFrame, ind_col: str, test_size: float, random_state: int):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.utils import resample\n",
    "\n",
    "    ###\n",
    "    df = df.copy()\n",
    "    \n",
    "    df_minclass = df[df[ind_col]==1]\n",
    "    df_majclass = df[df[ind_col]==0]\n",
    "    n_samples = len(df_majclass) # number of additional samples to generate\n",
    "    \n",
    "    df_minclass = resample(df_minclass, n_samples=n_samples, random_state=random_state)\n",
    "    \n",
    "    #combine majclass + minclass + minclass_upsampled\n",
    "    #df = pd.concat([df_minclass_upsampled, df_minclass, df_majclass], axis=0) #.sort_index()\n",
    "    df = pd.concat([df_minclass, df_majclass], axis=0) #.sort_index()\n",
    "    \n",
    "    X = df.drop(columns=[ind_col])\n",
    "    y = df[ind_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    ###\n",
    "\n",
    "### Demo function call\n",
    "\n",
    "demo_df_balance_encode_split_data = pd.DataFrame({\n",
    "    'Gross Commit Indicator': [0, 0, 0, 0, 1, 1, 1, 1, 0, 1],\n",
    "    \"Financial Aid Intent_FAN\": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0],\n",
    "    \"Financial Aid Intent_FAY\": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "    \"Scholarship_DIR\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "    \"Scholarship_LDRS\": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "    \"Scholarship_NO\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "    \"Scholarship_No Scholarship\": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"Ethnic Background_Asian\": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0],\n",
    "    \"Ethnic Background_Black\": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0],\n",
    "    \"Ethnic Background_White\": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    'GPA': [3.67, 3.76, 3.58, 4.0, 3.9, 3.72, 3.94, 3.76, 3.58, 3.67],\n",
    "    'HS Class Size': [80.0, 26.0, 642.0, 303.0, 288.0, 288.0, 77.0, 77.0, 303.0, 288.0],\n",
    "    \"Ethnic Background_Multiracial\": [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "    \"Permanent Region_CA\": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0],\n",
    "    \"Permanent Region_NY\": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0],\n",
    "    \"Permanent Region_WA\": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"Sex_F\": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0],\n",
    "    \"Sex_M\": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0],\n",
    "    \"Level of Financial Need_High\": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "    \"Level of Financial Need_Low\": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
    "    \"Level of Financial Need_Medium\": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "    \"Level of Financial Need_Very High\": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "})\n",
    "\n",
    "X_train, X_test, y_train, y_test = balance_split_data(\n",
    "    demo_df_balance_encode_split_data, \n",
    "    ind_col='Gross Commit Indicator', \n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_test.shape:  {X_test.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\"y_test.shape:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "balance_split_data.test_boilerplate"
    ]
   },
   "source": [
    " **Example:** A correct implementation should produce the following output:\n",
    "```python\n",
    "X_train.shape: (8, 21)\n",
    "X_test.shape:  (2, 21)\n",
    "y_train.shape: (8,)\n",
    "y_test.shape:  (2,)\n",
    "```\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for balance_split_data (exercise 6). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_6",
     "locked": true,
     "points": 3,
     "solution": false
    },
    "tags": [
     "balance_split_data.test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance_split_data test ran 30 iterations in 2.06 seconds\n",
      "Passed! Please submit.\n"
     ]
    }
   ],
   "source": [
    "### Test Cell - Exercise 6  \n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    execute_tests = dill.load(f)\n",
    "\n",
    "# Execute test\n",
    "passed, test_case_vars = execute_tests(func=plugins.series_handler(balance_split_data),\n",
    "              ex_name='balance_split_data',\n",
    "              key=b'apvdqSXE1hpoezgeyhgb6Y557k-QtNd5WaF1QCOuIQE=', \n",
    "              n_iter=30)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "\n",
    "assert passed, 'The solution to balance_split_data did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Permanent Region_ZG</th>\n",
       "      <th>Permanent Region_Kiev</th>\n",
       "      <th>Scholarship_MBS</th>\n",
       "      <th>Permanent Region_Hyogo-Ken</th>\n",
       "      <th>Permanent Region_SA</th>\n",
       "      <th>Permanent Region_DC</th>\n",
       "      <th>Permanent Region_London</th>\n",
       "      <th>Permanent Region_Seoul</th>\n",
       "      <th>Permanent Region_AL</th>\n",
       "      <th>Permanent Region_Abu Dhabi</th>\n",
       "      <th>Permanent Region_Shanghai</th>\n",
       "      <th>Permanent Region_Gauteng</th>\n",
       "      <th>Permanent Region_KA</th>\n",
       "      <th>Permanent Region_RM</th>\n",
       "      <th>Permanent Region_Nuevo Leon</th>\n",
       "      <th>Permanent Region_Jabotabek</th>\n",
       "      <th>Scholarship_GIRL</th>\n",
       "      <th>Permanent Region_AZ</th>\n",
       "      <th>Permanent Region_Gyeonggi-do</th>\n",
       "      <th>Permanent Region_Berkshire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2014 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Permanent Region_ZG  Permanent Region_Kiev  Scholarship_MBS  \\\n",
       "1593                  0.0                    0.0              0.0   \n",
       "1706                  0.0                    0.0              0.0   \n",
       "2365                  0.0                    0.0              0.0   \n",
       "491                   0.0                    0.0              0.0   \n",
       "355                   0.0                    0.0              0.0   \n",
       "...                   ...                    ...              ...   \n",
       "1122                  0.0                    0.0              0.0   \n",
       "728                   0.0                    0.0              0.0   \n",
       "723                   0.0                    0.0              0.0   \n",
       "1889                  0.0                    0.0              0.0   \n",
       "1933                  0.0                    0.0              0.0   \n",
       "\n",
       "      Permanent Region_Hyogo-Ken  Permanent Region_SA  Permanent Region_DC  \\\n",
       "1593                         0.0                  0.0                  0.0   \n",
       "1706                         0.0                  0.0                  0.0   \n",
       "2365                         0.0                  0.0                  0.0   \n",
       "491                          0.0                  0.0                  0.0   \n",
       "355                          0.0                  0.0                  0.0   \n",
       "...                          ...                  ...                  ...   \n",
       "1122                         0.0                  0.0                  0.0   \n",
       "728                          0.0                  0.0                  0.0   \n",
       "723                          0.0                  0.0                  0.0   \n",
       "1889                         0.0                  0.0                  0.0   \n",
       "1933                         0.0                  0.0                  0.0   \n",
       "\n",
       "      Permanent Region_London  Permanent Region_Seoul  Permanent Region_AL  \\\n",
       "1593                      0.0                     0.0                  0.0   \n",
       "1706                      0.0                     0.0                  0.0   \n",
       "2365                      0.0                     0.0                  0.0   \n",
       "491                       0.0                     0.0                  0.0   \n",
       "355                       0.0                     0.0                  0.0   \n",
       "...                       ...                     ...                  ...   \n",
       "1122                      0.0                     0.0                  0.0   \n",
       "728                       0.0                     0.0                  0.0   \n",
       "723                       0.0                     0.0                  0.0   \n",
       "1889                      0.0                     0.0                  0.0   \n",
       "1933                      0.0                     0.0                  0.0   \n",
       "\n",
       "      Permanent Region_Abu Dhabi  Permanent Region_Shanghai  \\\n",
       "1593                         0.0                        0.0   \n",
       "1706                         0.0                        0.0   \n",
       "2365                         0.0                        0.0   \n",
       "491                          0.0                        0.0   \n",
       "355                          0.0                        0.0   \n",
       "...                          ...                        ...   \n",
       "1122                         0.0                        0.0   \n",
       "728                          0.0                        0.0   \n",
       "723                          0.0                        0.0   \n",
       "1889                         0.0                        0.0   \n",
       "1933                         0.0                        0.0   \n",
       "\n",
       "      Permanent Region_Gauteng  Permanent Region_KA  Permanent Region_RM  \\\n",
       "1593                       0.0                  0.0                  0.0   \n",
       "1706                       0.0                  0.0                  0.0   \n",
       "2365                       0.0                  0.0                  0.0   \n",
       "491                        0.0                  0.0                  0.0   \n",
       "355                        0.0                  0.0                  0.0   \n",
       "...                        ...                  ...                  ...   \n",
       "1122                       0.0                  0.0                  0.0   \n",
       "728                        0.0                  0.0                  0.0   \n",
       "723                        0.0                  0.0                  0.0   \n",
       "1889                       0.0                  0.0                  0.0   \n",
       "1933                       0.0                  0.0                  0.0   \n",
       "\n",
       "      Permanent Region_Nuevo Leon  Permanent Region_Jabotabek  \\\n",
       "1593                          0.0                         0.0   \n",
       "1706                          0.0                         0.0   \n",
       "2365                          0.0                         0.0   \n",
       "491                           0.0                         0.0   \n",
       "355                           0.0                         0.0   \n",
       "...                           ...                         ...   \n",
       "1122                          0.0                         0.0   \n",
       "728                           0.0                         0.0   \n",
       "723                           0.0                         0.0   \n",
       "1889                          0.0                         0.0   \n",
       "1933                          0.0                         0.0   \n",
       "\n",
       "      Scholarship_GIRL  Permanent Region_AZ  Permanent Region_Gyeonggi-do  \\\n",
       "1593               0.0                  0.0                           0.0   \n",
       "1706               0.0                  0.0                           0.0   \n",
       "2365               0.0                  0.0                           0.0   \n",
       "491                0.0                  0.0                           0.0   \n",
       "355                0.0                  0.0                           0.0   \n",
       "...                ...                  ...                           ...   \n",
       "1122               0.0                  0.0                           0.0   \n",
       "728                0.0                  0.0                           0.0   \n",
       "723                0.0                  0.0                           0.0   \n",
       "1889               0.0                  0.0                           0.0   \n",
       "1933               0.0                  0.0                           0.0   \n",
       "\n",
       "      Permanent Region_Berkshire  \n",
       "1593                         0.0  \n",
       "1706                         0.0  \n",
       "2365                         0.0  \n",
       "491                          0.0  \n",
       "355                          0.0  \n",
       "...                          ...  \n",
       "1122                         0.0  \n",
       "728                          0.0  \n",
       "723                          0.0  \n",
       "1889                         0.0  \n",
       "1933                         0.0  \n",
       "\n",
       "[2014 rows x 20 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_output_vars['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Permanent Region_ZG</th>\n",
       "      <th>Permanent Region_Kiev</th>\n",
       "      <th>Scholarship_MBS</th>\n",
       "      <th>Permanent Region_Hyogo-Ken</th>\n",
       "      <th>Permanent Region_SA</th>\n",
       "      <th>Permanent Region_DC</th>\n",
       "      <th>Permanent Region_London</th>\n",
       "      <th>Permanent Region_Seoul</th>\n",
       "      <th>Permanent Region_AL</th>\n",
       "      <th>Permanent Region_Abu Dhabi</th>\n",
       "      <th>Permanent Region_Shanghai</th>\n",
       "      <th>Permanent Region_Gauteng</th>\n",
       "      <th>Permanent Region_KA</th>\n",
       "      <th>Permanent Region_RM</th>\n",
       "      <th>Permanent Region_Nuevo Leon</th>\n",
       "      <th>Permanent Region_Jabotabek</th>\n",
       "      <th>Scholarship_GIRL</th>\n",
       "      <th>Permanent Region_AZ</th>\n",
       "      <th>Permanent Region_Gyeonggi-do</th>\n",
       "      <th>Permanent Region_Berkshire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3312 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Permanent Region_ZG  Permanent Region_Kiev  Scholarship_MBS  \\\n",
       "2320                  0.0                    0.0              0.0   \n",
       "681                   0.0                    0.0              0.0   \n",
       "1464                  0.0                    0.0              0.0   \n",
       "2022                  0.0                    0.0              0.0   \n",
       "994                   0.0                    0.0              0.0   \n",
       "...                   ...                    ...              ...   \n",
       "412                   0.0                    0.0              0.0   \n",
       "825                   0.0                    0.0              0.0   \n",
       "705                   0.0                    0.0              0.0   \n",
       "1924                  0.0                    0.0              0.0   \n",
       "1600                  0.0                    0.0              0.0   \n",
       "\n",
       "      Permanent Region_Hyogo-Ken  Permanent Region_SA  Permanent Region_DC  \\\n",
       "2320                         0.0                  0.0                  0.0   \n",
       "681                          0.0                  0.0                  0.0   \n",
       "1464                         0.0                  0.0                  0.0   \n",
       "2022                         0.0                  0.0                  0.0   \n",
       "994                          0.0                  0.0                  0.0   \n",
       "...                          ...                  ...                  ...   \n",
       "412                          0.0                  0.0                  0.0   \n",
       "825                          0.0                  0.0                  0.0   \n",
       "705                          0.0                  0.0                  0.0   \n",
       "1924                         0.0                  0.0                  0.0   \n",
       "1600                         0.0                  0.0                  0.0   \n",
       "\n",
       "      Permanent Region_London  Permanent Region_Seoul  Permanent Region_AL  \\\n",
       "2320                      0.0                     0.0                  0.0   \n",
       "681                       0.0                     0.0                  0.0   \n",
       "1464                      0.0                     0.0                  0.0   \n",
       "2022                      0.0                     0.0                  0.0   \n",
       "994                       0.0                     0.0                  0.0   \n",
       "...                       ...                     ...                  ...   \n",
       "412                       0.0                     0.0                  0.0   \n",
       "825                       0.0                     0.0                  0.0   \n",
       "705                       0.0                     0.0                  0.0   \n",
       "1924                      0.0                     0.0                  0.0   \n",
       "1600                      0.0                     0.0                  0.0   \n",
       "\n",
       "      Permanent Region_Abu Dhabi  Permanent Region_Shanghai  \\\n",
       "2320                         0.0                        0.0   \n",
       "681                          0.0                        0.0   \n",
       "1464                         0.0                        0.0   \n",
       "2022                         0.0                        0.0   \n",
       "994                          0.0                        0.0   \n",
       "...                          ...                        ...   \n",
       "412                          0.0                        0.0   \n",
       "825                          0.0                        0.0   \n",
       "705                          0.0                        0.0   \n",
       "1924                         0.0                        0.0   \n",
       "1600                         0.0                        0.0   \n",
       "\n",
       "      Permanent Region_Gauteng  Permanent Region_KA  Permanent Region_RM  \\\n",
       "2320                       0.0                  0.0                  0.0   \n",
       "681                        0.0                  0.0                  0.0   \n",
       "1464                       0.0                  0.0                  0.0   \n",
       "2022                       0.0                  0.0                  0.0   \n",
       "994                        0.0                  0.0                  0.0   \n",
       "...                        ...                  ...                  ...   \n",
       "412                        0.0                  0.0                  0.0   \n",
       "825                        0.0                  0.0                  0.0   \n",
       "705                        0.0                  0.0                  0.0   \n",
       "1924                       0.0                  0.0                  0.0   \n",
       "1600                       0.0                  0.0                  0.0   \n",
       "\n",
       "      Permanent Region_Nuevo Leon  Permanent Region_Jabotabek  \\\n",
       "2320                          0.0                         0.0   \n",
       "681                           0.0                         0.0   \n",
       "1464                          0.0                         0.0   \n",
       "2022                          0.0                         0.0   \n",
       "994                           0.0                         0.0   \n",
       "...                           ...                         ...   \n",
       "412                           0.0                         0.0   \n",
       "825                           0.0                         0.0   \n",
       "705                           0.0                         0.0   \n",
       "1924                          0.0                         0.0   \n",
       "1600                          0.0                         0.0   \n",
       "\n",
       "      Scholarship_GIRL  Permanent Region_AZ  Permanent Region_Gyeonggi-do  \\\n",
       "2320               0.0                  0.0                           0.0   \n",
       "681                0.0                  0.0                           0.0   \n",
       "1464               0.0                  0.0                           0.0   \n",
       "2022               0.0                  0.0                           0.0   \n",
       "994                0.0                  0.0                           0.0   \n",
       "...                ...                  ...                           ...   \n",
       "412                0.0                  0.0                           0.0   \n",
       "825                0.0                  0.0                           0.0   \n",
       "705                0.0                  0.0                           0.0   \n",
       "1924               0.0                  0.0                           0.0   \n",
       "1600               0.0                  0.0                           0.0   \n",
       "\n",
       "      Permanent Region_Berkshire  \n",
       "2320                         0.0  \n",
       "681                          0.0  \n",
       "1464                         0.0  \n",
       "2022                         0.0  \n",
       "994                          0.0  \n",
       "...                          ...  \n",
       "412                          0.0  \n",
       "825                          0.0  \n",
       "705                          0.0  \n",
       "1924                         0.0  \n",
       "1600                         0.0  \n",
       "\n",
       "[3312 rows x 20 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_output_vars['X_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "calculate_f1_score.prompt"
    ]
   },
   "source": [
    "### Exercise 7: (2 points)\n",
    "**calculate_f1_score**  \n",
    "\n",
    "**Your task:** define `calculate_f1_score` as follows:\n",
    "\n",
    "\n",
    "Calculate the F1 score for a classification model's predictions.\n",
    "\n",
    "The F1 score is a metric that balances precision and recall, making it particularly useful for evaluating models on imbalanced datasets.\n",
    "\n",
    "**Input:**\n",
    "- `y_true`: A list or NumPy array of true class labels (must contain only 0 or 1).\n",
    "- `y_pred`: A list or NumPy array of predicted class labels (must contain only 0 or 1).\n",
    "\n",
    "**Output:**\n",
    "- `f1_score`: A float representing the F1 score, rounded to 3 decimal places.\n",
    "\n",
    "**Definitions:**\n",
    "- **True Positives (TP):** The number of correctly predicted positive class instances.\n",
    "- **False Positives (FP):** The number of instances incorrectly predicted as positive (but actually negative).\n",
    "- **False Negatives (FN):** The number of instances incorrectly predicted as negative (but actually positive).\n",
    "\n",
    "**Formulas:**\n",
    "1. **Precision ($P$):**\n",
    "    $$P = \\frac{TP}{TP + FP}$$\n",
    "    Precision measures the proportion of positive predictions that are correct.\n",
    "\n",
    "2. **Recall ($R$):**\n",
    "    $$R = \\frac{TP}{TP + FN}$$\n",
    "    Recall measures the proportion of actual positive instances that are correctly predicted.\n",
    "\n",
    "3. **F1 Score ($F1$):**\n",
    "    $$F1 = 2 \\cdot \\frac{P \\cdot R}{P + R}$$\n",
    "    The F1 score is the harmonic mean of precision and recall.\n",
    "\n",
    "**Special Cases:**\n",
    "- If $TP + FP = 0$ (no positive predictions), precision is undefined.\n",
    "- If $TP + FN = 0$ (no actual positives), recall is undefined.\n",
    "- In both cases, set the F1 score to 0 to handle division by zero.\n",
    "\n",
    "**Hints:**\n",
    "- Use NumPy's vectorized operations (e.g., `np.sum`) to efficiently calculate $TP$, $FP$, and $FN$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "calculate_f1_score.solution"
    ]
   },
   "outputs": [],
   "source": [
    "### Solution - Exercise 7  \n",
    "def calculate_f1_score(y_true, y_pred):\n",
    "\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "\n",
    "### Demo function call\n",
    "demo_y_true_f1_score = np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 0])\n",
    "demo_y_pred_f1_score = np.array([0, 1, 1, 0, 1, 0, 1, 0, 0, 1])\n",
    "\n",
    "demo_output_f1_score = calculate_f1_score(demo_y_true_f1_score, demo_y_pred_f1_score)\n",
    "print(demo_output_f1_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "calculate_f1_score.test_boilerplate"
    ]
   },
   "source": [
    " **Example**: A correct implementation should produce the following output:\n",
    "\n",
    "```python\n",
    "# Example Input\n",
    "y_true = [0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
    "y_pred = [0, 1, 1, 0, 1, 0, 1, 0, 0, 1]\n",
    "\n",
    "# Example Output\n",
    "F1 Score: 0.8\n",
    "```\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for calculate_f1_score (exercise 7). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_7",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "calculate_f1_score.test"
    ]
   },
   "outputs": [],
   "source": [
    "### Test Cell - Exercise 7  \n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    execute_tests = dill.load(f)\n",
    "\n",
    "# Execute test\n",
    "passed, test_case_vars = execute_tests(func=calculate_f1_score,\n",
    "              ex_name='calculate_f1_score',\n",
    "              key=b'apvdqSXE1hpoezgeyhgb6Y557k-QtNd5WaF1QCOuIQE=', \n",
    "              n_iter=100)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "\n",
    "assert passed, 'The solution to calculate_f1_score did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "compute_logistic_metrics.prompt"
    ]
   },
   "source": [
    "### Exercise 8: (2 points)\n",
    "**compute_logistic_metrics**  \n",
    "\n",
    "**Your task:** define `compute_logistic_metrics` as follows:\n",
    "\n",
    "\n",
    "Calculate the predictions and gradients required for training a logistic regression model.\n",
    "\n",
    "**Input:**\n",
    "- `X`: A NumPy array of shape (m, n+1) containing the input features. The first column must be all ones, representing the bias term. Each row represents a single observation, and each column represents a feature.\n",
    "- `y_true`: A NumPy array of shape (m,) containing the true class labels (0 or 1) for each observation.\n",
    "- `weights`: A NumPy array of shape (n+1,) representing the model weights, including the bias term.\n",
    "\n",
    "**Output:**\n",
    "- `sigmoid_predictions`: A NumPy array of shape (m,) containing the predicted probabilities for each observation. These values are computed using the sigmoid function.\n",
    "- `gradients_w`: A NumPy array of shape (n+1,) containing the gradient of the cost with respect to each weight, including the bias term.\n",
    "\n",
    "**Requirements/steps:**\n",
    "1. **Compute the Linear Combination of Inputs**:\n",
    "    - Calculate $z$, the linear combination of inputs, using the formula:\n",
    "    $$z = X \\cdot weights$$\n",
    "2. **Compute Sigmoid Predictions**:\n",
    "    - Apply the sigmoid function to $z$ to compute predicted probabilities:\n",
    "    $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "    - Note: These predicted probabilities, $\\sigma(z)$, are referred to as $\\hat{y}$ in logistic regression. \n",
    "3. **Compute Gradients**:\n",
    "    - Calculate the gradient of the cost with respect to the weights:\n",
    "    $$\\frac{\\partial J}{\\partial w} = \\frac{1}{m} X^T (\\hat{y} - y)$$\n",
    "    - Here, $\\hat{y}$ represents the predicted probabilities (sigmoid predictions), and $m$ is the number of samples in the dataset.\n",
    "\n",
    "**Hints:**\n",
    "- Use NumPy's vectorized operations for efficient computation of gradients.\n",
    "- Note that $X$ includes the bias term as the first column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "compute_logistic_metrics.solution"
    ]
   },
   "outputs": [],
   "source": [
    "### Solution - Exercise 8  \n",
    "def compute_logistic_metrics(X, y_true, weights):\n",
    "\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "\n",
    "### Demo function call\n",
    "# Example Input\n",
    "demo_X_logistic_metrics = np.array([[1, 1, 2], [1, 3, 4], [1, 5, 6]])  # Includes the bias column\n",
    "demo_y_true_logistic_metrics = np.array([0, 1, 1])\n",
    "demo_weights_logistic_metrics = np.array([0.5, 0.1, 0.2])  # Includes bias as the first weight\n",
    "\n",
    "# Compute Outputs\n",
    "demo_sigmoid_predictions, demo_gradients_w = compute_logistic_metrics(\n",
    "    demo_X_logistic_metrics, demo_y_true_logistic_metrics, demo_weights_logistic_metrics\n",
    ")\n",
    "\n",
    "# Print Outputs\n",
    "print(f\"Sigmoid Predictions: {demo_sigmoid_predictions}\")\n",
    "print(f\"Gradients (Weights): {demo_gradients_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "compute_logistic_metrics.test_boilerplate"
    ]
   },
   "source": [
    " **Example**: A correct implementation should produce the following output:\n",
    "\n",
    "   ```python\n",
    "   # Example Input\n",
    "   X = np.array([[1, 1, 2], [1, 3, 4], [1, 5, 6]])  # Includes the bias column as the first column\n",
    "   y_true = np.array([0, 1, 1])\n",
    "   weights = np.array([0.5, 0.1, 0.2])\n",
    "\n",
    "   # Example Output\n",
    "   Sigmoid Predictions: array([0.73105858 0.83201839 0.90024951])\n",
    "   Gradients (Weights): array([ 0.15444216 -0.09054624  0.06389592])\n",
    "   ```\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for compute_logistic_metrics (exercise 8). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_8",
     "locked": true,
     "points": 2,
     "solution": false
    },
    "tags": [
     "compute_logistic_metrics.test"
    ]
   },
   "outputs": [],
   "source": [
    "### Test Cell - Exercise 8  \n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    execute_tests = dill.load(f)\n",
    "\n",
    "# Execute test\n",
    "passed, test_case_vars = execute_tests(func=compute_logistic_metrics,\n",
    "              ex_name='compute_logistic_metrics',\n",
    "              key=b'apvdqSXE1hpoezgeyhgb6Y557k-QtNd5WaF1QCOuIQE=', \n",
    "              n_iter=100)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "\n",
    "assert passed, 'The solution to compute_logistic_metrics did not pass the test.'\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fin\n",
    "If you have made it this far, congratulations! You are done. **Please submit your exam!**\n",
    "\n",
    "The remainder of this notebook combines the work you have completed above with a few addition steps to build a working logistic regression model.\n",
    "\n",
    "### Epilogue: It's Time to Build a Model\n",
    "\n",
    "```python\n",
    "## Data Exploration and Cleaning\n",
    "\n",
    "# Step 0: Fill in missing values and standardize feature formats\n",
    "cleaned_df = data_cleaning_and_standardization(admission_df)\n",
    "\n",
    "# Step 1: Create clean state_data dictionary\n",
    "abbr_dict = utils.load_object_from_publicdata('abbr_dict.dill')\n",
    "coor_dict = utils.load_object_from_publicdata('coor_dict.dill')\n",
    "state_data = (abbr_dict, coor_dict)\n",
    "\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "# Step 2: Use state_data to feature engineer new columns in cleaned_df\n",
    "lat_long_df = add_lat_long(cleaned_df, state_data)\n",
    "\n",
    "# Step 3: Add student distance from school\n",
    "distance_df = calculate_distance(lat_long_df, 34.1271, -118.2109)\n",
    "\n",
    "# Step 4: One-hot encode categorical features\n",
    "one_hot_df = one_hot_encode(distance_df)\n",
    "\n",
    "## Model Building and Evaluation\n",
    "\n",
    "# Step 5: Split data into train and test while fixing class imbalance\n",
    "X_train, X_test, y_train, y_test = balance_split_data(one_hot_df, 'Gross Commit Indicator', 0.2, 42)\n",
    "\n",
    "# Step 6: Train logistic regression model using (exercise is left to the reader)\n",
    "def train_logistic_regression(X_train, y_train):\n",
    "    # Initialize the weights and biases to zero\n",
    "    \n",
    "    # Implement the cost and gradients from previous exercises\n",
    "    \n",
    "    # Use gradient desent to update the weights and biases\n",
    "\n",
    "  return weights, bias\n",
    "  \n",
    "\n",
    "# Step 7: Make predictions\n",
    "def predict_logistic_regression(X_train, y_train, X_test):\n",
    "    weights, bias = train_logistic_regression(X_train, y_train)\n",
    "    \n",
    "    # Apply sigmoid function to compute predicted probabilities\n",
    "    y_pred = \n",
    "    \n",
    "    y_pred_class = (y_pred >= 0.5).astype(int)\n",
    "    return y_pred_class\n",
    "y_pred = predict_logistic_regression(X_train, y_train, X_test)\n",
    " \n",
    "# Step 8: Evaluate the model\n",
    "f1_score = calculate_f1_score(y_test, y_pred)\n",
    "print(f\"Model F1 Score: {f1_score}\")\n",
    "\n",
    "Model F1 Score: 0.595\n",
    "```\n",
    "\n",
    "Our model converges!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
